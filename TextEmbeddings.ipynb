{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f37a2da-c16d-48df-8f71-1c6c7f46e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "094151bf-e4ce-4bc6-8166-d5d932fe319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350206c0-a793-4e38-aa78-66b5574ad0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Read the Text Data\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f98c0be-80ef-4bcc-bdf0-3864eba9eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenize the Text\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", '', text)  # Remove punctuation\n",
    "    words = text.split()  # Split by whitespace\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc63933-527b-40dc-b8e0-8293b26d9cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create Vocabulary\n",
    "def build_vocab(words):\n",
    "    word_counts = Counter(words)\n",
    "    vocabulary = {word: idx for idx, (word, _) in enumerate(word_counts.items())}\n",
    "    idx_to_word = {idx: word for word, idx in vocabulary.items()}\n",
    "    return vocabulary, idx_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5eac4d6-6486-41f7-b387-0278db308061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare Input Data for Skip-Gram Model\n",
    "def prepare_data(words, vocabulary, window_size=2):\n",
    "    data = []\n",
    "    for idx, word in enumerate(words):\n",
    "        for neighbor in range(-window_size, window_size + 1):\n",
    "            if neighbor == 0:\n",
    "                continue\n",
    "            neighbor_idx = idx + neighbor\n",
    "            if 0 <= neighbor_idx < len(words):\n",
    "                data.append((vocabulary[word], vocabulary[words[neighbor_idx]]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81b771f3-59e9-4ea1-afee-94d9eb6d3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define the Neural Network Model\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, center_word):\n",
    "        embed = self.embeddings(center_word)\n",
    "        out = self.output_layer(embed)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0052cd4d-58fa-4f1f-a109-bb97e4dd71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create batches\n",
    "def create_batches(data, batch_size):\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(data)\n",
    "    # Split the data into batches\n",
    "    batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4150e72b-bbd6-48d3-a928-e18a31439357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model with Batching\n",
    "def train(skip_gram_model, data, vocab_size, embedding_dim, batch_size=64, epochs=10, learning_rate=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(skip_gram_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Move the model to the device (GPU or CPU)\n",
    "    skip_gram_model.to(device)\n",
    "\n",
    "    # Create batches\n",
    "    batches = create_batches(data, batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in batches:\n",
    "            # Separate center words and context words in the batch\n",
    "            center_batch = [pair[0] for pair in batch]\n",
    "            context_batch = [pair[1] for pair in batch]\n",
    "\n",
    "            # Convert to tensors and move to device\n",
    "            center_batch = torch.LongTensor(center_batch).to(device)\n",
    "            context_batch = torch.LongTensor(context_batch).to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = skip_gram_model(center_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, context_batch)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "182fab9d-23bb-4db7-ad64-eb1027c1c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = 'sample_text.txt'  # Replace with your .txt file path\n",
    "text = load_text(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a047c86b-a499-4ef3-b2e6-3cda38ba22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "words = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56d9da7e-9d73-451d-8528-e3733bda1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "vocabulary, idx_to_word = build_vocab(words)\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ad6b8d7-1157-401e-bbe7-3709e8b7682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "data = prepare_data(words, vocabulary, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "501d52c5-f34f-4a79-adbc-a6ec2dc19bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Skip-Gram model\n",
    "embedding_dim = 50  # You can adjust the embedding dimension\n",
    "skip_gram_model = SkipGramModel(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5fbe89a-4fd7-4f2d-9377-e3ac198730b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 17.392043113708496\n",
      "Epoch 2, Loss: 15.375497341156006\n",
      "Epoch 3, Loss: 14.320851802825928\n",
      "Epoch 4, Loss: 13.572887420654297\n",
      "Epoch 5, Loss: 12.954366445541382\n",
      "Epoch 6, Loss: 12.481980562210083\n",
      "Epoch 7, Loss: 12.137553453445435\n",
      "Epoch 8, Loss: 11.87052297592163\n",
      "Epoch 9, Loss: 11.650390386581421\n",
      "Epoch 10, Loss: 11.470478057861328\n",
      "Epoch 11, Loss: 11.329429864883423\n",
      "Epoch 12, Loss: 11.216997146606445\n",
      "Epoch 13, Loss: 11.124237298965454\n",
      "Epoch 14, Loss: 11.046266794204712\n",
      "Epoch 15, Loss: 10.980922937393188\n",
      "Epoch 16, Loss: 10.9269540309906\n",
      "Epoch 17, Loss: 10.882646560668945\n",
      "Epoch 18, Loss: 10.846586227416992\n",
      "Epoch 19, Loss: 10.817251443862915\n",
      "Epoch 20, Loss: 10.792980432510376\n",
      "Epoch 21, Loss: 10.772388935089111\n",
      "Epoch 22, Loss: 10.754582166671753\n",
      "Epoch 23, Loss: 10.7390878200531\n",
      "Epoch 24, Loss: 10.725602626800537\n",
      "Epoch 25, Loss: 10.713829517364502\n",
      "Epoch 26, Loss: 10.703450202941895\n",
      "Epoch 27, Loss: 10.694203853607178\n",
      "Epoch 28, Loss: 10.685911655426025\n",
      "Epoch 29, Loss: 10.678441286087036\n",
      "Epoch 30, Loss: 10.671688556671143\n",
      "Epoch 31, Loss: 10.665560483932495\n",
      "Epoch 32, Loss: 10.659986972808838\n",
      "Epoch 33, Loss: 10.654906988143921\n",
      "Epoch 34, Loss: 10.650257587432861\n",
      "Epoch 35, Loss: 10.645979166030884\n",
      "Epoch 36, Loss: 10.642025709152222\n",
      "Epoch 37, Loss: 10.638365507125854\n",
      "Epoch 38, Loss: 10.634968519210815\n",
      "Epoch 39, Loss: 10.631808757781982\n",
      "Epoch 40, Loss: 10.628865003585815\n",
      "Epoch 41, Loss: 10.62611436843872\n",
      "Epoch 42, Loss: 10.623541593551636\n",
      "Epoch 43, Loss: 10.621128797531128\n",
      "Epoch 44, Loss: 10.618863105773926\n",
      "Epoch 45, Loss: 10.616730690002441\n",
      "Epoch 46, Loss: 10.614720582962036\n",
      "Epoch 47, Loss: 10.612821578979492\n",
      "Epoch 48, Loss: 10.611025333404541\n",
      "Epoch 49, Loss: 10.609324216842651\n",
      "Epoch 50, Loss: 10.607710838317871\n"
     ]
    }
   ],
   "source": [
    "# Train the model with batching\n",
    "batch_size = 64  # Set the batch size\n",
    "train(skip_gram_model, data, vocab_size, embedding_dim, batch_size=batch_size, epochs=50, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e2347f9-c560-4299-a2c2-3db0f9589728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipGramModel(\n",
       "  (embeddings): Embedding(32, 50)\n",
       "  (output_layer): Linear(in_features=50, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training, move embeddings back to CPU for further processing if needed\n",
    "skip_gram_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce607825-4b0a-49d4-90b1-6c1a92dde605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: the, Embedding: [-0.14251852 -0.94881225 -0.5413972  -1.2912344  -0.13615991 -0.97734463\n",
      "  0.22281218 -1.1646893   0.41605696  0.5624783  -1.215951    0.5887805\n",
      "  0.43272135 -0.46438175 -1.5950586   0.42580098 -0.26277116 -0.9383325\n",
      " -0.37033722  0.41089806  0.35435766  0.87726593 -0.00665058 -0.65032375\n",
      " -0.1435545   1.3600202  -0.5079986  -1.5001851   0.48574454 -0.08523705\n",
      " -0.01757985  0.7027134   0.687775    1.7696964  -0.3522051   0.9554862\n",
      " -0.24144818  0.12313724 -0.24975005  0.24052599 -1.318385   -1.3535063\n",
      " -0.26460585  1.0759817  -0.6620209  -0.4765183   1.3182598  -0.8869704\n",
      "  1.6224129  -1.7740642 ]\n",
      "Word: cat, Embedding: [ 2.1221316  -0.32849133  1.2511393  -0.38994083  0.96975464 -0.8020615\n",
      "  0.8452099   1.6009694   0.1035975   0.98592174 -1.2350359   0.07670219\n",
      "  0.628      -0.16801563  0.98632765 -2.680964   -0.12100766 -1.3112888\n",
      " -2.2745821   0.427327   -0.14534713  0.19691992  0.25935045 -0.8978313\n",
      "  0.6531155  -0.13397418 -0.5384055   1.2730356  -0.68806404 -0.46674687\n",
      "  0.05522723  1.1664009  -2.3017473  -0.3643803  -1.7928755  -2.46506\n",
      " -2.3207679   0.4917348  -1.0954224  -0.31284273  1.8153671   0.8572397\n",
      "  0.10635252 -0.04215356 -0.5266184   0.50444204 -0.76706654  0.6565284\n",
      "  1.4452105  -0.7998248 ]\n",
      "Word: sat, Embedding: [ 2.599699   -0.00437845  0.8527648   0.24992085 -0.5731989  -0.3171639\n",
      " -0.65589124 -0.9888715   0.730192    0.48333028 -0.4636933   0.14379734\n",
      " -2.0917764  -0.64908093  0.69051397 -0.48892385  0.94273365 -0.1972151\n",
      " -0.49205038 -1.6453454   1.6145391   1.6167718   1.1757427   1.1948435\n",
      " -0.5926353  -2.7074165   0.8083722  -0.9016373  -0.12997973  1.1534001\n",
      " -1.4525168   0.38227078  0.07024861  0.6259215  -0.63837427 -0.8279704\n",
      "  0.23011352  0.76689965 -0.29650027 -0.02126939  0.3977252   2.1094396\n",
      " -0.30395085  1.279794    0.12569816  0.89039433  1.8394125  -1.2620115\n",
      "  0.602553   -1.8342053 ]\n",
      "Word: on, Embedding: [-0.04340016  0.13212179  0.12761815 -0.25676876  1.6440719   0.44391823\n",
      "  0.85239863 -0.72613037 -0.19200414  0.4336728   1.5920367   0.569338\n",
      " -0.2137849  -2.1831338  -1.4024174  -0.30131003  0.25842893 -0.3711371\n",
      " -1.0094727   2.042544    1.5214038   0.30044088 -0.4581316   2.3785145\n",
      "  0.1602855  -0.6470939   0.6317042   1.8863052  -1.135017   -0.95593494\n",
      " -1.7543526   0.20915404  0.15724048  0.2708272   0.09715425 -0.89638\n",
      "  0.01540317  1.8307666  -0.30261108 -0.88865536  0.22851104 -1.4215524\n",
      "  0.87936956  1.199121    0.90791553  0.7217123   0.03278153  0.6753441\n",
      "  0.19104409 -1.1192046 ]\n",
      "Word: mat, Embedding: [ 0.608982   -1.6581994   1.3038812  -1.1974443   1.2146673   0.11568075\n",
      " -0.6120398  -1.262934    1.2541567  -1.1798422  -0.39911878 -0.52423036\n",
      "  1.377972    0.34631538 -0.55696166 -0.5462492  -0.5595002  -2.6956954\n",
      "  0.69486934  0.41368335 -0.83158046  1.0225124   1.5029061   1.4574332\n",
      "  0.6814566   1.0498552  -2.0421896   1.1463692   1.0781983   0.2184332\n",
      "  0.41189158  0.97378665  0.8527991  -0.34007272 -1.3096318  -0.3391886\n",
      "  0.36633673  0.5250286  -0.80047876  0.22999617  0.24720553  0.5874509\n",
      " -0.58888483  1.3407679  -1.029029    0.6573062  -0.98526275  0.41905737\n",
      " -0.94277215 -1.0046273 ]\n",
      "Word: dog, Embedding: [ 1.8031291   1.4249961   1.2222104  -0.8885161   0.5961472  -0.1616278\n",
      "  1.6952835  -0.16217877 -1.1325799   1.1119032  -0.25770226  0.5534931\n",
      "  0.8891684  -1.5895114   0.27154827 -1.659096   -1.8889948   1.171417\n",
      " -1.5807796   0.92462    -0.78242683 -2.0681736   1.5005314  -0.651563\n",
      "  1.6799728  -2.7144134  -0.2833614   0.00757241  1.3490864  -1.0465522\n",
      "  0.05389878  3.5271535   0.30881256 -1.2905915  -0.29652676 -0.73094946\n",
      "  0.22095463  0.53717065 -1.5120937   0.7030507  -0.3787771   0.99503696\n",
      " -0.80933046 -0.32021403 -0.88691217  0.4472521  -0.66649544 -1.2674985\n",
      "  1.908796    0.38188237]\n",
      "Word: lay, Embedding: [ 2.0020587   0.96464926  0.4499474   0.9840733   0.59533536 -0.65540314\n",
      " -0.3778977   0.26740944 -0.16968079 -0.60761297  0.32503474  0.63305503\n",
      " -0.9031229   0.288946   -0.76030934 -1.2368466   0.6161087   0.22628297\n",
      "  0.29241177  1.2819694   0.21741918  1.266306    2.048418    0.5398593\n",
      "  0.8019829  -0.79956824 -1.0319204  -0.66011345  1.0051265  -1.3754206\n",
      " -1.3181646   1.6115444  -0.08394009  0.57643545 -0.8202374  -0.56234956\n",
      " -0.5036005  -0.02948983 -0.6406173   0.30819595  1.2455035  -0.48742795\n",
      " -0.43130106  0.60723263 -0.79588264  2.4490063  -0.72030467 -0.28597173\n",
      " -0.5658437  -0.24678653]\n",
      "Word: rug, Embedding: [ 1.4328114  -0.55746984 -0.13574465  1.2021712  -1.039107   -1.5478433\n",
      "  1.3694259   0.69059974 -0.40626594 -1.189163   -1.1638585  -0.682631\n",
      "  0.32662946  1.060727    1.0858564  -1.5892042   0.9586304  -0.37839088\n",
      " -1.7736852  -0.52580494  1.0778967  -0.20467961 -0.19474721  0.78992844\n",
      "  1.2189971  -0.7619833  -0.9248158   0.5330836   1.4306653   1.375539\n",
      " -1.4072396  -1.0131197   1.1116139   0.1809096  -1.3735335  -1.9341449\n",
      " -0.8969124  -0.09677441  0.5208832   0.02012061  1.5452725  -1.1036626\n",
      " -0.04463328 -1.354511   -1.4752953   0.03514066 -0.8585436  -0.6363006\n",
      " -0.6592526   0.88753176]\n",
      "Word: bird, Embedding: [-0.05503086  0.51063883  0.5769694  -0.04450375  0.6497195  -1.7884668\n",
      "  0.90601873  0.38567093  0.25194442 -0.41806644  0.824841   -0.02736595\n",
      " -1.920567    1.4118278  -1.7744889   1.6591495  -0.20851594 -0.00786174\n",
      "  0.86109734 -1.3084265  -0.07889029 -1.8274115   0.70161456 -0.9444962\n",
      " -0.2109954  -0.34874853 -1.606672    0.4343148  -1.9913398   1.8338507\n",
      " -0.29377478  0.8444126  -0.63392     1.1877884   0.2240042  -0.3945502\n",
      "  1.627005    0.91224766  0.78711736 -0.79382175  0.43370438 -0.31777263\n",
      " -1.1344752  -1.0309995   0.15006839 -0.7511639  -0.6756818  -1.5769585\n",
      "  1.5376859  -0.7363589 ]\n",
      "Word: flew, Embedding: [-0.03611532 -0.13438226 -0.11935943 -1.4733454  -0.4835582   0.08984831\n",
      "  0.6089935   1.8786236  -0.30345026 -2.1504142   0.91398543 -0.41779813\n",
      "  1.1284875  -0.13933903  0.06129666  0.47545716 -0.10835662 -0.99748755\n",
      " -1.7903368  -0.4120965  -0.5752654  -0.82527816  2.4740782   0.2290828\n",
      "  0.44895977  0.00924342  0.6620105  -1.6388279   1.5672921   1.4934629\n",
      "  0.22189838 -0.8500013  -1.2061092   0.5437192  -0.6849283  -1.761112\n",
      "  0.7948928  -0.4037819  -0.27581683  0.25006592  0.11709395 -0.46439478\n",
      "  2.4767344  -0.02065999 -2.4815738   0.29436302  0.2541173  -1.5291771\n",
      "  1.5497346  -1.9460196 ]\n",
      "Word: over, Embedding: [ 1.2779068   1.3503193   0.01711967 -0.1683753   1.2519276   1.8340809\n",
      " -0.52939266 -0.06701637  0.8498514  -2.2096438  -1.480461    2.611014\n",
      " -1.8212464   0.9898512   1.0241268  -0.2333735  -1.3810334  -0.36275753\n",
      "  2.7912405   0.73480505  1.115281    1.6663043   2.6481485  -0.5520795\n",
      "  1.1406723  -1.8593289  -0.56684303 -1.4433725   0.60070115  1.8931253\n",
      " -2.0378644  -0.04141885 -1.4077096   0.56505454  1.2268556   1.6164286\n",
      " -0.18226661 -1.25596    -1.1505628  -1.9322075   1.4629016  -1.2469852\n",
      "  0.60630983 -1.0567371  -1.2874751  -0.32443973 -1.4102713  -1.2806106\n",
      " -0.01792948  1.9050804 ]\n",
      "Word: trees, Embedding: [-0.723709   -0.20158178  2.176916   -0.23133253  0.9744527  -0.43166575\n",
      " -1.0310931  -1.613493   -1.8521478   0.59384114  1.3122962  -1.0071976\n",
      "  0.01739766  0.68780994 -0.30979124  0.9299332  -0.2859552  -3.493573\n",
      " -0.87800574  0.08115812  0.07672112 -0.66251785  1.6308334  -1.1117774\n",
      "  0.4159595  -0.58728653  0.96760833 -0.92670876  0.8365363  -0.57227033\n",
      " -0.94611907  0.13712257 -0.67941195  0.97359073 -0.5369222   0.857836\n",
      "  1.2105503   0.49967626 -0.12903878  0.5965643   0.281839    0.15462635\n",
      "  0.32564414 -1.8760586  -0.65211785 -1.4527869   0.6964085  -0.58530563\n",
      "  0.8723502  -1.2443517 ]\n",
      "Word: chased, Embedding: [ 0.2619935   0.03411068  0.93218106 -1.3295584   0.97442985 -1.9019421\n",
      " -2.4512434  -1.2285258   0.06518932 -0.54158866 -0.27247447  1.8248717\n",
      "  1.0768385  -0.10485882  0.51920617 -0.373055   -0.18682848 -0.2631868\n",
      " -0.5116089   0.39782053  0.40558895 -0.9774823   0.3292171   0.27596456\n",
      " -0.03353426  0.7347384   1.8050439   1.3547581   0.21666287  0.70159745\n",
      " -1.3415215   0.7986275   0.52115494  2.327907   -0.07549885 -0.7894243\n",
      "  0.27274156  0.29866064 -1.1405779  -0.17494407  0.9591531  -1.6819065\n",
      "  0.15510808 -0.71761     0.4014959  -1.2034427   0.9339876  -1.3974779\n",
      "  1.3194804   1.1876161 ]\n",
      "Word: barked, Embedding: [ 0.5622743   0.31983766  2.1134262  -0.48366812  0.28751633 -0.05329893\n",
      " -0.90344137  1.0999655   1.4680274  -1.2275121  -0.73601115  1.0639325\n",
      " -0.13694346  0.27427852  0.1306581  -2.362621    0.16589472 -0.73221964\n",
      " -1.0639338  -0.40004542  1.4110974  -0.43401706 -0.5202444  -0.04794694\n",
      " -0.83001536  0.24427406 -0.45391276  2.076094    1.2680627  -0.642666\n",
      " -0.01181931 -2.089104    1.5654298   0.43973863 -2.3858423  -0.65829873\n",
      "  0.38496825  0.38292053  0.41522115 -2.5479472   1.329275   -0.68965834\n",
      " -0.5825262  -1.9171038   0.49002913  0.13121508 -0.9139009  -1.1344861\n",
      "  0.17699331  0.5476917 ]\n",
      "Word: at, Embedding: [ 0.21707919  0.2386281  -1.2265247   0.1606277  -1.304773   -0.31462902\n",
      " -2.6995056   0.5558674   0.32172465  1.8683513   1.5848672   0.7889289\n",
      " -0.73642004  0.44871894 -0.37373748 -0.38152933 -1.9925187  -0.55872506\n",
      " -0.9982774  -1.2528225   1.0366567  -0.21391402 -2.1145635   3.4004436\n",
      " -0.809078   -0.56632245 -1.014547   -1.4130335  -0.6705608   0.48474416\n",
      " -0.60964745  0.5154377  -0.26216692  1.7892326  -0.4183516  -0.29337177\n",
      "  0.9239497  -0.81644326  0.17353182  0.5737887  -0.9033377  -1.2251339\n",
      " -1.7298086   0.7754491  -1.9576756   0.9914667  -0.09939348 -1.5588008\n",
      " -0.81994945 -0.80891097]\n",
      "Word: sang, Embedding: [ 9.5781970e-01 -2.1113265e-01 -1.0789200e+00 -1.1018043e+00\n",
      "  4.5098621e-01  1.3525134e+00 -7.5463301e-01 -1.4980115e+00\n",
      " -2.6667702e-01 -9.6848816e-01 -5.6765676e-01  1.0677177e+00\n",
      " -1.5009288e+00 -1.4236522e+00 -7.9227054e-01 -2.2333311e-01\n",
      "  7.6452279e-01  8.8926417e-01  2.2067784e-01 -1.7252777e+00\n",
      " -1.1982071e+00 -1.2886775e+00  1.9820462e+00 -5.7706410e-01\n",
      "  4.2667171e-01  2.3709054e-01  3.4955999e-01  5.9987855e-01\n",
      "  5.9243900e-01  1.0294955e+00  2.5566684e-02  4.3372372e-01\n",
      "  1.0083605e+00  3.0416071e+00 -1.9160199e+00  4.2717129e-01\n",
      " -1.4864554e-04  1.4465432e+00 -1.5203151e+00 -7.8469008e-01\n",
      "  3.2974538e-01  2.9226609e-02 -7.1684128e-01 -1.0718063e+00\n",
      " -1.8274992e+00 -7.9451197e-01  9.5291871e-01 -6.5738416e-01\n",
      "  2.1074584e-01  5.0592607e-01]\n",
      "Word: in, Embedding: [-6.9664842e-01 -5.6105804e-01 -2.3730397e-01 -8.2980245e-01\n",
      "  1.6557447e+00  1.2649539e+00 -5.4264373e-01  1.5425967e+00\n",
      " -7.0823050e-01 -6.9604117e-01  7.7275276e-02 -7.4871466e-02\n",
      "  6.3832295e-01  3.3767307e-01  5.3299761e-01 -1.1806324e+00\n",
      " -3.2480156e+00  9.6501261e-01 -1.4641923e-01 -4.2731950e-01\n",
      " -9.5086366e-01  1.0633068e-01  8.4576243e-01  1.0253966e+00\n",
      " -9.7999647e-02  1.5903386e+00  1.8994081e+00 -1.5186533e+00\n",
      "  1.1657426e+00 -3.3122268e-01 -1.2950152e+00 -5.9514767e-01\n",
      " -2.4384844e+00  8.2051897e-01 -5.2639949e-01  6.0711171e-02\n",
      "  8.0519223e-01  8.2909875e-02 -2.1285620e+00  1.1860600e+00\n",
      "  4.7817484e-02 -2.3442940e-04 -1.2740002e+00  1.1287431e+00\n",
      " -9.3958348e-01 -4.4582237e-02  5.9520747e-03  4.6358123e-01\n",
      "  2.0214267e-01 -1.8419478e+00]\n",
      "Word: tree, Embedding: [-0.31498492 -1.5097013   0.0636928   1.3602382  -0.76506495  2.1221566\n",
      "  1.5427266  -1.0330238  -0.4115446  -0.92276925  2.3454895   0.97957814\n",
      "  0.23553927 -0.49604234 -0.66459703  0.49343768  0.24363796 -1.0836542\n",
      "  0.09400108 -1.5361521   1.516265   -1.187341    0.05632508  0.8330758\n",
      "  0.5630169   1.9710268   1.4863765  -1.3908933  -0.8320952   0.42902386\n",
      "  1.4624354   0.05914925  1.5899421  -1.3003738  -0.9872001  -0.3655914\n",
      "  0.49136338 -0.711411   -0.6959389  -1.0044532   0.5953637   0.8012843\n",
      " -0.5050432  -0.9789093  -0.5388609  -3.1828024   2.7834146  -1.8577828\n",
      "  1.2193991  -0.9077083 ]\n",
      "Word: was, Embedding: [ 1.3817507   0.6124523   1.9339213   0.3240148   0.19196656  2.240144\n",
      " -0.27586895  0.20740686 -0.9220316  -0.40809664  0.32441124  0.15147723\n",
      "  1.0979214  -0.06630853 -1.1802962   0.56529677 -0.55447406 -0.3678848\n",
      "  0.66499    -0.15581353  0.7785356   2.434571   -0.39097285  1.0027945\n",
      "  0.38971868  1.8329084  -0.05132938  0.0991745  -1.1921955   0.30520892\n",
      " -0.8765863   1.5902205   1.1735197   0.8906813   0.02900182 -0.09867117\n",
      " -0.26503566  0.8463564   1.0781317  -1.0302263  -1.3874226  -1.6057084\n",
      " -0.68706894 -1.311197    0.67856    -1.7416416  -1.199353    0.43272072\n",
      " -0.80853975  0.6089465 ]\n",
      "Word: next, Embedding: [ 0.46938086  1.3779807  -0.6308671   0.6363472   2.8993227  -1.3849083\n",
      " -0.68670195 -0.21800604  0.13582124 -1.4423074   0.77599084 -1.3503658\n",
      "  0.6257691  -2.9557488   0.69222885  1.2312043  -0.45297226 -2.2109954\n",
      "  1.9757365  -0.36555108 -0.61933064 -1.0918841   0.37211242  1.181239\n",
      "  0.43443993 -0.81762123  1.063385   -1.6713451  -1.1047996  -0.81342745\n",
      " -1.5352644   0.8644385   1.4042404   0.38799983  1.8565538  -0.34514368\n",
      "  0.3295185   0.87768036 -0.22991501  0.15242246 -0.41468334 -0.6688481\n",
      " -1.8562316  -2.465804    0.6587815   1.8199342   0.48818582 -0.17681293\n",
      " -0.17412648 -0.02443468]\n",
      "Word: to, Embedding: [ 0.7074788   1.1549425  -1.0805874  -1.2134222  -1.5446056   0.14444314\n",
      " -2.2744038   1.5571842   0.23557949 -1.7640252   0.16755491 -0.99658954\n",
      "  1.4436187  -0.2881754  -0.33577287  0.34940588 -1.348569   -1.3767035\n",
      "  0.6737432   0.25285107 -0.66807026 -0.9332751   0.14265096 -0.18204528\n",
      " -0.43995932  0.44925907 -1.7006439  -0.29595017 -0.4319403   1.031101\n",
      "  0.3284598   0.39707476  1.2300204  -0.7172774   0.08161719 -0.74351364\n",
      " -1.5419053   2.126311    0.0589941   0.90610707  0.8116437  -0.20511505\n",
      "  1.7529452   1.9537816  -0.13421859 -1.2223895  -0.03377022  0.69751865\n",
      "  0.84017104 -1.4763157 ]\n",
      "Word: and, Embedding: [-0.06715301 -0.16895993  0.16883896  0.19509585 -0.7765175   0.10616536\n",
      "  0.5245957   1.1969965  -1.1598765  -1.2703685  -0.6282875   0.9500557\n",
      " -1.0778463  -1.6349512   0.95557153 -0.44489765 -1.2586998  -1.1747824\n",
      " -0.7624361  -0.21561041 -1.6016295  -0.56419945 -0.18608701 -1.631589\n",
      "  1.1014215   0.48124522 -0.9614923  -0.8719455  -0.41037577 -0.15651725\n",
      " -2.0660558   1.6034564   0.97669375 -0.62384933 -1.962755    0.50347364\n",
      "  1.0108011   2.6679852  -1.7856369   1.0289272  -0.719006   -3.0502074\n",
      " -1.4071369   0.34243533 -0.4848622   0.8518862  -0.64706516 -1.0278327\n",
      " -1.3599488   0.6082973 ]\n",
      "Word: slept, Embedding: [ 0.431409    0.41889057 -1.122141    0.24575569  1.2653612   0.70614016\n",
      " -0.06934522 -1.5399456  -1.2020324   0.3872869   1.7182405  -0.52711505\n",
      " -0.01733815  2.5367277  -1.2790439   0.31747025 -0.22362007 -0.07607488\n",
      "  0.2809253  -2.3435564   2.2427044   1.5841118   0.3361447   1.1339966\n",
      " -0.864914    0.1508046   1.1793203   0.04972386 -1.5504869   0.2958231\n",
      " -2.5528648   1.7472702  -1.343669   -0.5026568  -0.17645393  1.4617385\n",
      " -0.78499985  1.3332003   0.36535266  0.1415204   1.5335456  -0.4449287\n",
      " -0.73670316 -0.8267879   0.06973489  0.26655525 -0.47604     1.7830758\n",
      "  0.45404053 -0.78813297]\n",
      "Word: together, Embedding: [-0.75627357 -0.59881943 -0.17955777  0.60294104 -0.5462638   1.8620785\n",
      " -1.2136397  -0.4598603  -2.3999608  -0.9550816   0.17660654 -0.06755893\n",
      "  1.8346442   1.080049    0.49576294  0.3036067   1.1235224  -1.9264774\n",
      " -0.6517581   1.0867823   1.4395971  -0.31934395  0.60838485  0.16922559\n",
      "  0.0956441  -0.19754095 -0.61824524  0.06939629 -0.05732847  0.56546104\n",
      " -0.82241756 -0.08333606  0.38969225  0.21861634 -1.323991    0.3775813\n",
      " -0.3474067   1.7015309   0.4700932  -0.74065286  0.49710748  0.9253865\n",
      "  1.0603961   1.2275263   2.2265313   0.46348184  0.63179815 -0.7526566\n",
      "  0.35974377 -1.5399748 ]\n",
      "Word: played, Embedding: [-0.9576062  -0.7253051   1.4814533   0.03351228  1.5608402   0.6014249\n",
      "  0.17613362  1.2923231   0.7723314  -0.1544426   2.6418517  -1.2814866\n",
      " -0.28869408 -1.5162171   1.1213776  -2.9542282  -1.1342958  -0.66452366\n",
      " -0.83865356  0.05492597  1.3520354  -2.255086   -0.8122364  -0.04750828\n",
      " -1.611326   -1.3365368   0.11906875 -1.3716936   0.74732816 -0.6616106\n",
      " -0.19845203  0.558973    1.8372724  -0.04753406 -0.02538331  1.6096377\n",
      " -0.7463677   1.2989378  -0.7706968   0.27066746  1.4788866  -1.0350032\n",
      "  0.45362929 -0.19716066  1.0416158   0.43801922  0.26527816 -2.4389088\n",
      " -1.0840272  -0.02298598]\n",
      "Word: near, Embedding: [-0.806647    1.1624582   0.38977274  0.88065404 -0.34964952  0.17235668\n",
      " -1.0785031  -0.6975109  -0.34199962 -1.5284865   0.46715072  0.87906367\n",
      "  0.42666182 -0.90909433 -1.727709   -2.0566463  -1.0869502  -0.9161682\n",
      "  0.03050698  1.2816964   1.0033375   1.6093408   0.695001    2.2538984\n",
      " -1.3942338   0.23081842 -0.49476993 -0.58679354  1.2198077   0.7307373\n",
      " -0.10251608 -0.60228246  0.23513383 -0.10434144  1.1410333  -0.59344316\n",
      "  0.25435776 -0.5386552   2.0245726  -2.7879748   0.39937386  0.1449878\n",
      " -0.6104976   1.7578493   0.23136957 -0.7929042  -0.04986801 -0.7101735\n",
      " -0.11833007 -1.7206796 ]\n",
      "Word: were, Embedding: [ 1.8432457  -0.36127812  1.3217396  -1.6290249   0.23158532  0.7852098\n",
      "  0.8226931  -1.153953    0.36913607 -0.28626105  1.426049   -0.6247584\n",
      " -0.29791883  1.9273885  -2.9516027  -0.65538234 -1.626293   -1.0652895\n",
      " -1.253558   -1.2970117   0.2427629   0.8154959  -1.3036847  -2.2978313\n",
      " -0.6122105   0.7831055   0.19951977  1.0450561   0.07493577  0.6035689\n",
      " -0.39408267  0.8869592  -0.4681049  -0.27001533  0.8274946  -0.14743875\n",
      "  1.0468377   0.5328878  -1.1773252  -0.32864797  2.0816283  -0.81251055\n",
      "  1.5823313  -0.43091524  0.3609733   2.4176636  -1.5715574  -0.14562462\n",
      " -1.4900748  -0.8351505 ]\n",
      "Word: dirty, Embedding: [-0.21350165  1.4270139   0.96889395 -0.25996143 -1.1874596   2.054658\n",
      " -0.16088594 -0.7892343  -1.9936326  -1.7000606   0.26398396 -0.7095062\n",
      " -0.73634756  0.47034022  1.2184669  -3.1978688   0.29938492 -0.07016265\n",
      " -0.31286478 -1.2420621  -1.4397188  -1.6790031   0.21398637 -0.28771663\n",
      " -1.1526837   3.1193137  -0.904438    0.53229165  0.795896   -0.5897268\n",
      " -0.219846   -0.8368703  -0.19397195 -1.2756233  -0.90895617 -0.32336882\n",
      "  0.05925801  0.0544193  -0.59362394  1.3194587  -0.3481849  -2.625624\n",
      " -0.73202455  0.06868184 -0.34865323  1.7809876   0.35650098 -0.53721386\n",
      "  1.9014118  -0.51860946]\n",
      "Word: tall, Embedding: [ 1.048258   -0.66633934  1.3106521   1.4991457   0.75640863  1.8594805\n",
      "  0.93066573  0.6339426  -0.30774567 -0.8925797   0.4518152   1.6973454\n",
      "  1.3176984   1.2705562   0.5260718  -0.7368306  -0.05874484 -2.1432881\n",
      "  0.467564   -0.9519749  -0.421363    0.2596822   0.9952605   2.1202624\n",
      "  0.87843263  0.6479783   0.8047248  -1.1282344   0.13009988  0.948303\n",
      "  0.12855336  0.07574192  1.9591149   0.60457504  0.32503587  0.9180917\n",
      "  1.7248272  -2.5368469   0.3113993   2.0489705  -0.01121771  0.29963872\n",
      " -1.0782775  -0.2705989  -0.2158048  -0.31854582  0.9205195  -1.0229146\n",
      "  0.35328388 -0.37475824]\n",
      "Word: liked, Embedding: [-1.4001579   1.8517094  -1.1652086  -0.20873174  0.9555349   0.29255435\n",
      " -0.6794025  -0.9258185  -0.7772854  -0.71136653 -1.0675392  -0.61891323\n",
      "  1.2042711  -0.22958393  2.034399   -0.24742433 -1.1158955   0.5718577\n",
      "  0.35764146 -1.683979    1.5649395  -0.7953043  -0.08663025  1.5457354\n",
      "  1.2684927   0.758383   -0.798241    0.29978663 -0.6682536  -2.3193638\n",
      " -1.1108947  -0.8393389   1.4706055   1.4072185  -0.44137952 -1.356614\n",
      "  0.47392976  1.4499003  -0.46001098 -0.579208    0.03590911 -0.04492371\n",
      "  1.3829112  -1.1886178   1.2920091   0.0170292  -0.07854313  0.04511038\n",
      " -0.26824257 -0.2179185 ]\n",
      "Word: sit, Embedding: [ 0.92558366 -0.25789243  0.1984785  -1.5182681  -1.2955611   0.25238618\n",
      "  0.4687285   1.4076641  -1.9821256   1.1898614   0.27197516 -1.9216535\n",
      "  2.1812184   1.1247846   1.4522933   0.47011805  2.063676    2.1026075\n",
      "  1.2693304  -1.382199    0.68457    -0.5089334   0.5954454   0.12990463\n",
      "  1.1860253   0.2685962   1.5762577  -0.49238393  0.4614972  -0.42173123\n",
      " -2.3033435  -0.832568   -0.7169676  -0.46516523 -0.8560689   0.866006\n",
      " -0.09502642 -0.38361883 -0.62342304 -0.8861079   1.5482746   0.5369945\n",
      "  0.01695999  0.79847723  2.0786374   1.9708059   0.36674643  0.52406585\n",
      " -3.260137    0.14675856]\n",
      "Word: it, Embedding: [ 0.275346    0.9027248   0.3309981   0.11766588 -1.101682   -0.12521733\n",
      " -0.0311568   0.00295902  2.3071759  -0.8223879  -1.9200631   0.9923167\n",
      " -0.43227777  0.9972262  -0.20434178  2.722476    0.72400045  0.21904585\n",
      "  2.572765   -1.0437994   1.0102632  -1.325899   -0.44679853  0.96764475\n",
      " -1.2465471  -2.0358043   0.7398152   1.1257745   1.0319515  -1.0205203\n",
      " -1.8918862  -0.29628637 -1.0081785   1.4131786   0.5728509   0.66431415\n",
      " -1.0049856   2.3078651  -1.2921797   1.1854976  -0.22658783  1.3516229\n",
      " -0.00697828  1.2680086  -1.2992628   0.22736597  0.20169258  1.3169608\n",
      "  0.23389675  0.21721315]\n"
     ]
    }
   ],
   "source": [
    "# Get Embeddings\n",
    "word_embeddings = skip_gram_model.embeddings.weight.data.numpy()\n",
    "for word, idx in vocabulary.items():\n",
    "    print(f\"Word: {word}, Embedding: {word_embeddings[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67b626e6-fc12-40a8-89e9-fb339f0d1f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d291924-1fe2-4408-9551-1c69409ed647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between cat and dog : 0.41268414\n"
     ]
    }
   ],
   "source": [
    "# Compute the cosine similarity between two words\n",
    "# Extract the indices for the words \"once\" and \"upon\"\n",
    "word1 = 'cat'\n",
    "word2 = 'dog'\n",
    "idx_once = vocabulary.get(word1)\n",
    "idx_upon = vocabulary.get(word2)\n",
    "\n",
    "# Check if both words are in the vocabulary\n",
    "if idx_once is None or idx_upon is None:\n",
    "    print(\"One or both words are not in the vocabulary.\")\n",
    "else:\n",
    "    # Get the embeddings for \"once\" and \"upon\"\n",
    "    embedding_once = word_embeddings[idx_once]\n",
    "    embedding_upon = word_embeddings[idx_upon]\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    cosine_similarity = np.dot(embedding_once, embedding_upon) / (np.linalg.norm(embedding_once) * np.linalg.norm(embedding_upon))\n",
    "\n",
    "    # Print the cosine similarity\n",
    "    print(\"Cosine Similarity between\",word1,\"and\",word2,\":\", cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5918308b-d9e4-4e3a-9b53-929b3d0c4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Function to get the nearest k words in the embedding space\n",
    "def get_nearest_k_words(word, word_embeddings, vocabulary, idx_to_word, k=5):\n",
    "    # Check if the word exists in the vocabulary\n",
    "    if word not in vocabulary:\n",
    "        print(f\"'{word}' is not in the vocabulary.\")\n",
    "        return\n",
    "    \n",
    "    # Get the embedding of the given word\n",
    "    word_idx = vocabulary[word]\n",
    "    word_embedding = word_embeddings[word_idx]\n",
    "    \n",
    "    # Compute the cosine similarity between the given word and all other words\n",
    "    similarities = []\n",
    "    for other_word, other_idx in vocabulary.items():\n",
    "        if other_word != word:  # Skip comparing the word to itself\n",
    "            other_embedding = word_embeddings[other_idx]\n",
    "            #print(word_embedding)\n",
    "            #print(other_embedding)\n",
    "            similarity = cosine_similarity(word_embedding, other_embedding)\n",
    "            similarities.append((other_word, similarity))\n",
    "    \n",
    "    # Sort by similarity (highest first) and get the top k nearest words\n",
    "    nearest_words = sorted(similarities, key=lambda x: x[1], reverse=True)[:k]\n",
    "    \n",
    "    # Print the nearest words and their cosine similarity scores\n",
    "    print(f\"Nearest {k} words to '{word}':\")\n",
    "    for other_word, similarity in nearest_words:\n",
    "        print(f\"{other_word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7b910d2-5382-4a83-93b6-bf93a7517a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest 5 words to 'tree':\n",
      "tall: 0.4421\n",
      "trees: 0.2252\n",
      "sang: 0.2089\n",
      "flew: 0.1869\n",
      "together: 0.1708\n"
     ]
    }
   ],
   "source": [
    "word = 'tree'  # Example word to find nearest neighbors\n",
    "k = 5  # Number of nearest neighbors to find\n",
    "get_nearest_k_words(word, word_embeddings, vocabulary, idx_to_word, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a52ded-8e3d-4ebc-84da-3dbdebc349fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
